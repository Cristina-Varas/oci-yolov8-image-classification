{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Notes (Sanitized for Git)\n",
        "\n",
        "This repository contains a **sanitized** version of the Gracity Insects YOLOv8 Classification notebooks.\n",
        "All tenant-specific identifiers (bucket names, namespaces, OCIDs, local absolute paths) have been replaced by placeholders.\n",
        "\n",
        "**Author:** Cristina Varas Menadas  \n",
        "**Last updated:** 2026-02-19\n",
        "\n",
        "> To run these notebooks, set the configuration values in the first \"Configuration\" section of each notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339fac16",
      "metadata": {},
      "source": [
        "# Gracity Insects â€” 04. Evaluation & Metrics\n",
        "\n",
        "Checks:\n",
        "- Accuracy on validation\n",
        "- Per-class report + confusion matrix\n",
        "- Recall for mosquito class (if present)\n",
        "- Latency p50/p95 on local runtime\n",
        "\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Update these variables for your tenancy/project.\n",
        "\n",
        "- **Bucket**: `<BUCKET_NAME>`\n",
        "- **Dataset prefix** (images): `<PROJECT_PREFIX>/v1/raw/datasets/insects_kaggle_v1/`\n",
        "- **Labels prefix** (metadata/manifests): `<PROJECT_PREFIX>/v1/labels/insects_kaggle_v1/`\n",
        "- **Runs prefix** (artifacts): `<PROJECT_PREFIX>/yolo/runs/insects_kaggle_v1/`\n",
        "\n",
        "We intentionally keep **`test/` as validation** for this starter project (to match your current bucket structure)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6576940e",
      "metadata": {},
      "source": [
        "## 4.1 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ab7015",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad450efb",
      "metadata": {},
      "source": [
        "## 4.2 Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b7edb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_ROOT: str = \"<LOCAL_PATH> Gracity/gracity-insects-yolo-cls/outputs/dataset\"  # <-- update\n",
        "VAL_DIR = Path(DATASET_ROOT) / \"test\"\n",
        "\n",
        "RUN_DIR: str = \"./runs\"\n",
        "RUN_NAME: str = \"\"  # <-- set your run name\n",
        "WEIGHTS_PATH: Path = Path(RUN_DIR) / RUN_NAME / \"weights\" / \"best.pt\"\n",
        "\n",
        "assert VAL_DIR.exists(), VAL_DIR\n",
        "assert WEIGHTS_PATH.exists(), WEIGHTS_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa10b9f",
      "metadata": {},
      "source": [
        "## 4.3 Load validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7791d380",
      "metadata": {},
      "outputs": [],
      "source": [
        "def iter_images(val_dir: Path) -> List[Tuple[Path, str]]:\n",
        "    items: List[Tuple[Path, str]] = []\n",
        "    for class_dir in sorted([p for p in val_dir.iterdir() if p.is_dir()]):\n",
        "        for img_path in class_dir.iterdir():\n",
        "            if img_path.is_file():\n",
        "                items.append((img_path, class_dir.name))\n",
        "    return items\n",
        "\n",
        "val_items = iter_images(VAL_DIR)\n",
        "len(val_items), val_items[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d027868a",
      "metadata": {},
      "source": [
        "## 4.4 Predict + accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605d99b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = YOLO(str(WEIGHTS_PATH))\n",
        "\n",
        "y_true: List[str] = []\n",
        "y_pred: List[str] = []\n",
        "\n",
        "for img_path, label in val_items:\n",
        "    out = model.predict(str(img_path), verbose=False)\n",
        "    probs = out[0].probs\n",
        "    pred_name = model.names[int(probs.top1)]\n",
        "    y_true.append(label)\n",
        "    y_pred.append(pred_name)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(\"Validation accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee90e18",
      "metadata": {},
      "source": [
        "## 4.5 Report + confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12699272",
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = sorted(list(set(y_true) | set(y_pred)))\n",
        "print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix (Validation)\")\n",
        "plt.xticks(range(len(labels)), labels, rotation=90)\n",
        "plt.yticks(range(len(labels)), labels)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137a4585",
      "metadata": {},
      "source": [
        "## 4.6 Requirement checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12057168",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Meets accuracy >= 0.85 ?\", acc >= 0.85)\n",
        "\n",
        "mos_label_candidates = {\"Mosquito\", \"MOS\"}\n",
        "mos_label = next((c for c in mos_label_candidates if c in set(y_true)), None)\n",
        "\n",
        "if mos_label is None:\n",
        "    print(\"Mosquito class not found; skipping MOS recall.\")\n",
        "else:\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    tp = cm_df.loc[mos_label, mos_label]\n",
        "    fn = cm_df.loc[mos_label, :].sum() - tp\n",
        "    recall_mos = float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0\n",
        "    print(f\"Recall({mos_label}) =\", recall_mos)\n",
        "    print(\"Meets MOS recall >= 0.80 ?\", recall_mos >= 0.80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa0c99c",
      "metadata": {},
      "source": [
        "## 4.7 Latency p50/p95 (local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a67ea9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "sample_paths = [p for p,_ in val_items[:50]]\n",
        "lat_ms: List[float] = []\n",
        "\n",
        "for p in sample_paths:\n",
        "    t0 = time.perf_counter()\n",
        "    _ = model.predict(str(p), verbose=False)\n",
        "    lat_ms.append((time.perf_counter() - t0) * 1000.0)\n",
        "\n",
        "p50 = statistics.median(lat_ms)\n",
        "p95 = float(np.percentile(lat_ms, 95))\n",
        "print(f\"Latency ms p50: {p50:.1f}\")\n",
        "print(f\"Latency ms p95: {p95:.1f}\")"
      ]
    }
  ],
  "metadata": {
    "created": "2026-02-18T11:43:23.580120Z",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "title": "04 Evaluate Metrics & Latency"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}